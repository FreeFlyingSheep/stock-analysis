# Stock Analysis

Stock Analysis is a comprehensive A-share fundamental scoring agent for Chinese stock analysis. It provides data crawling from multiple sources (CNInfo, Yahoo Finance), rule-based scoring, filtering, and a RESTful API for querying results.

## Disclaimer

**This tool is for reference and educational purposes only. The data and analysis provided are for informational purposes and should not be considered as financial advice.**

I do **NOT** provide any financial, investment, or trading recommendations. Users should conduct their own research and consult with qualified financial advisors before making any investment decisions.

**Investment involves risk, including possible loss of principal.** Past performance is not indicative of future results. Use this tool at your own risk and discretion.

## Features

- **Stock Data Management**: Store and query Chinese A-share stock information with classifications and industries
- **Multi-source Data Crawling**: Fetch data from CNInfo and Yahoo Finance with automatic rate limiting and retries
- **Rule-based Scoring**: Declarative YAML rules for computing financial metrics and overall scores
- **Stock Filtering**: Filter stocks based on configurable scoring rules
- **Analysis Results Storage**: Persist computed metrics and scores to database
- **RESTful API**: FastAPI-based async API with automatic Swagger/ReDoc documentation
- **Job Queue**: PgQueuer for async job processing (data crawling and analysis)
- **Async Database Operations**: SQLAlchemy async with PostgreSQL for high performance
- **CSV Import**: Bulk import stock data from CSV files
- **Pagination & Filtering**: Query stocks with filtering by classification/industry and pagination
- **Database Migrations**: Alembic integration for schema versioning
- **Type Safety**: Full type hints with Pylance validation
- **Google Docstrings**: Comprehensive documentation throughout the codebase
- **Code Quality**: Type checking and formatting standards

## Features TODO List

High priority:

- Develop frontend dashboard for visualizing stock data and scores
- Integrate LLM for advanced stock analysis and explanations
- Add a chatbot interface for querying stock information

Medium priority:

- Expand data source integrations

Low priority:

- Avoid duplicate jobs in PgQueuer
- Add observability and monitoring with logging aggregation
- Deploy with Docker and Kubernetes

## Requirements

- Python 3.14+
- PostgreSQL 18+
- [uv](https://docs.astral.sh/uv/) package manager

## Installation

1. Clone the repository:

    ```bash
    git clone https://github.com/FreeFlyingSheep/stock-analysis
    cd stock-analysis
    ```

2. Install dependencies using uv:

    ```bash
    uv sync
    ```

3. Configure environment variables:

    ```bash
    cp .env.example .env
    # Edit .env with your database credentials and settings
    ```

4. Initialize the database:

    ```bash
    ./scripts/init.sh
    ```

    This script will:
    - Drop any existing database
    - Create a fresh database
    - Run Alembic migrations to create tables
    - Import initial stock data from `data/stocks.csv`
    - Initialize the PgQueuer job queue

## Usage

### Starting the API Server

Run the FastAPI application with hot reload:

```bash
uv run app
```

The API will be available at `http://127.0.0.1:8000`.

API documentation is automatically generated by FastAPI and can be accessed at:

- Swagger UI: `http://127.0.0.1:8000/docs`
- ReDoc: `http://127.0.0.1:8000/redoc`

### Running the Job Queue

In a separate terminal, run the PgQueuer to process crawl and analysis jobs:

```bash
./scripts/run-pgq.sh
```

This will start the job processor that:

- Fetches stock data from CNInfo and Yahoo Finance
- Computes scores and metrics using configured rules
- Stores results in the database

### API Endpoints

**Stock Operations** (`/stocks`):

- `GET /stocks` - List stocks with pagination and filtering
  - Query parameters: `page`, `size`, `classification`, `industry`
- `GET /stocks/{stock_code}` - Get detailed stock information with API responses
  - Automatically queues data crawl if not cached

**Analysis Operations** (`/analysis`):

- `GET /analysis` - List analysis results with pagination
  - Query parameters: `page`, `size`
- `GET /analysis/{stock_code}` - Get analysis scores for a specific stock
  - Automatically queues analysis job if not computed

### Configuration

Edit `.env` to configure:

- Database connection details (`DATABASE_*`)
- Server settings (`HOST`, `PORT`)
- Logging (`LOG_LEVEL`, `LOG_FILE`)
- Rule file path (`RULE_FILE_PATH`)
- Debug mode (`DEBUG`)

## Project Structure

```text
stock-analysis/
├── configs/              # Configuration YAML files
│   ├── api/             # API endpoint specifications
│   │   ├── cninfo/      # CNInfo endpoint configs
│   │   └── yahoo/       # Yahoo Finance configs
│   └── rules/           # Scoring rule definitions
├── data/                # Data and sample responses
│   ├── stocks.csv       # Initial stock data
│   └── api/             # Sample API responses
├── scripts/             # Utility scripts
│   ├── init.sh          # Initialize database
│   ├── run.sh           # Run job queue
│   ├── create_db.py     # Create database
│   ├── drop_db.py       # Drop database
│   ├── import_csv.py    # Import CSV data
│   └── check.sh         # Code quality checks
├── src/                 # Source code
│   └── stock_analysis/
│       ├── main.py      # Application entry point
│       ├── settings.py  # Configuration management
│       ├── logger.py    # Logging setup
│       ├── models/      # SQLAlchemy ORM models
│       ├── schemas/     # Pydantic schemas
│       ├── adaptors/    # Data source adaptors
│       ├── services/    # Business logic services
│       ├── jobs/        # Async job handlers
│       └── routers/     # FastAPI route handlers
├── tests/               # Test suite
├── pyproject.toml       # Project configuration
└── README.md            # This file
```

## Development

### Running Tests

```bash
uv run pytest
```

### Code Quality Checks

```bash
./scripts/check.sh
```

This runs:

- Type checking with Pylance
- Formatting validation with Ruff
- Linting with Ruff

### Database Management

**Drop existing database:**

```bash
uv run scripts/drop_db.py
```

**Create database:**

```bash
uv run scripts/create_db.py
```

**Run migrations:**

```bash
uv run alembic upgrade head
```

**Create new migration:**

```bash
uv run alembic revision --autogenerate -m "description"
```

### Importing Stock Data

To import stock data from a CSV file:

```bash
uv run scripts/import_csv.py
```

The CSV file should have columns matching the Stock model:

- `stock_code`: Unique identifier
- `company_name`: Company name
- `classification`: Stock classification
- `industry`: Industry sector

## Architecture

### Data Flow

1. **Data Crawling**: PgQueuer jobs fetch data from CNInfo and Yahoo Finance
2. **API Adaptors**: Validate parameters and handle HTTP requests with retries
3. **Database Storage**: Raw API responses stored in dedicated tables
4. **Rule Engine**: Computes metrics from raw data using YAML rules
5. **Analysis**: Generates scores and stores analysis results
6. **API Access**: RESTful endpoints provide access to stocks and analysis

### Key Components

- **Adaptors**: Handle external API integrations (CNInfo, Yahoo Finance)
- **Services**: Implement business logic (stock queries, analysis)
- **Jobs**: Async tasks for data processing (crawling, analysis)
- **Routers**: FastAPI endpoints for HTTP API
- **Models**: SQLAlchemy ORM models for database
- **Schemas**: Pydantic models for request/response validation

## Configuration Files

### API Specifications

See [configs/api/README.md](configs/api/README.md) for details on:

- Adding new API endpoints
- YAML structure and conventions
- Maintaining endpoint specifications

### Scoring Rules

See [configs/rules/README.md](configs/rules/README.md) for details on:

- Writing scoring rules
- Metric definitions
- Filter specifications
- Rule versioning

### Sample Data

See [data/api/README.md](data/api/README.md) for details on:

- API response samples
- Adding new response examples
- Maintaining sample data

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
